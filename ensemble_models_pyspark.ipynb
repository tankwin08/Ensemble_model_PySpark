{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble models - PySpark\n",
    "\n",
    "\n",
    "## Goal\n",
    "To examplify the uses of ensemble models in PySpark as the ensemble models in [previous project using sklearn and keras](https://github.com/tankwin08/ensemble-models-ML-DL-) and predict if the client will subscribe (yes/no) a term deposit (variable y) using market campaign data.\n",
    "\n",
    "\n",
    "## Ensemble models\n",
    "\n",
    "Ensemble modeling is a process where multiple diverse models are created to predict an outcome, either by using many different modeling algorithms or using different training data sets. The ensemble model then aggregates the prediction of each base model and results in once final prediction for the unseen data. The motivation for using ensemble models is to reduce the generalization error of the prediction. As long as the base models are diverse and independent, the prediction error of the model decreases when the ensemble approach is used.\n",
    "\n",
    "The approach seeks the wisdom of crowds in making a prediction. Even though the ensemble model has multiple base models within the model, it acts and performs as a single model.\n",
    "\n",
    "A signle model generally suffers from high bias or high variance due to data quality, train and test data drift, distribution of hypothesis...\n",
    "\n",
    "The goal of the modelling to find a method wit low bias and low variance. Thus, it is common to **aggregate** several base models to provide solutions.\n",
    "\n",
    "### Aggregate strategies\n",
    "\n",
    "There are multiple ways to conduct aggregation and improve the model performance either from accuracy or robustness. \n",
    "\n",
    "**1 Baggging**\n",
    "\n",
    "The bagging strategy is built on the bootstrap sampling. In short, it built multiple classifer independently and in parallel using data derived from resampling from the training set. Then aggregate these classifiers using average processing or major vote to redce the variability of prediction. However the accuracy/point estimate is not improved.\n",
    "\n",
    "\n",
    "**2 Boosting**\n",
    "\n",
    " \n",
    "The boosting is similar to bagging strategy to some extent in terms of resampling methods. But it differs in two major ways:\n",
    "\n",
    "    1 how trees are built: The Bagging method builds each tree independently while Boosting method builds one tree at a time. This additive model (ensemble) works in a forward stage-wise manner, introducing a weak learner to improve the shortcomings of existing weak learners. \n",
    "    \n",
    "    2 Results combination: The Bagging method combine results at the end of the process (by averaging or \"majority rules\") while the boosting combines results along the way.\n",
    "    \n",
    "If you carefully tune parameters, boosting can result in better performance than bagging. However, boosting may not be a good choice if you have a lot of noise, as it can result in overfitting. They also tend to be harder to tune than bagging method.\n",
    "\n",
    "\n",
    "**3 Stacking**\n",
    "\n",
    "Stacking provide a whole new different way to combine classifers. There are two major differences:\n",
    "\n",
    "    1 stacking often considers heterogeneous weak learners (different learning algorithms are combined) whereas bagging and boosting consider mainly homogeneous weak learners.\n",
    "    \n",
    "    2 The stacking uses a second layer model which uses the predictions of weak classifiers such as bagging and bostting results as input.\n",
    "\n",
    "\n",
    "In this project, the stacking strategy was used to predict if the client will subscribe (yes/no) a term deposit (variable y) using market campaign data.\n",
    "\n",
    "\n",
    "## Data\n",
    "\n",
    "The data used for this project can be downloaded from [here](https://archive.ics.uci.edu/ml/machine-learning-databases/00222/).\n",
    "\n",
    "The explination of the data can be found [here](https://archive.ics.uci.edu/ml/datasets/bank+marketing).\n",
    "\n",
    "The data is related with **direct marketing campaigns of a Portuguese banking institution**. The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be ('yes') or not ('no') subscribed. \n",
    "\n",
    "### Input variables:\n",
    "\n",
    "**bank client data:**\n",
    "\n",
    "    1 - age (numeric)\n",
    "    2 - job : type of job (categorical: 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')\n",
    "    3 - marital : marital status (categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed)\n",
    "    4 - education (categorical: 'basic.4y','basic.6y','basic.9y','high.school','illiterate','professional.course','university.degree','unknown')\n",
    "    5 - default: has credit in default? (categorical: 'no','yes','unknown')\n",
    "    6 - housing: has housing loan? (categorical: 'no','yes','unknown')\n",
    "    7 - loan: has personal loan? (categorical: 'no','yes','unknown')\n",
    "    \n",
    "** related with the last contact of the current campaign:**\n",
    "\n",
    "    8 - contact: contact communication type (categorical: 'cellular','telephone') \n",
    "    9 - month: last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')\n",
    "    10 - day_of_week: last contact day of the week (categorical: 'mon','tue','wed','thu','fri')\n",
    "    11 - duration: last contact duration, in seconds (numeric). \n",
    "    \n",
    "**Important note:** Attribute 11 highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.\n",
    "\n",
    "\n",
    "**other attributes:**\n",
    "\n",
    "    12 - campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)\n",
    "    13 - pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)\n",
    "    14 - previous: number of contacts performed before this campaign and for this client (numeric)\n",
    "    15 - poutcome: outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success')\n",
    "\n",
    "**social and economic context attributes**\n",
    "\n",
    "    16 - emp.var.rate: employment variation rate - quarterly indicator (numeric)\n",
    "    17 - cons.price.idx: consumer price index - monthly indicator (numeric) \n",
    "    18 - cons.conf.idx: consumer confidence index - monthly indicator (numeric) \n",
    "    19 - euribor3m: euribor 3 month rate - daily indicator (numeric)\n",
    "    20 - nr.employed: number of employees - quarterly indicator (numeric)\n",
    "\n",
    "**Output variable (desired target):**\n",
    "\n",
    "    21 - y - has the client subscribed a term deposit? (binary: 'yes','no')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "## References\n",
    "\n",
    "[Machine Learning Case Study with Spark: Make it better](http://people.stat.sc.edu/haigang/improvement.html).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup pyspark\n",
    "\n",
    "If this is your first time to set up pyspark, please follow the [intructions](https://medium.com/@naomi.fridman/install-pyspark-to-run-on-jupyter-notebook-on-windows-4ec2009de21f) to set it up properly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.classification import  RandomForestClassifier\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoderEstimator, VectorAssembler, VectorSlicer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext()\n",
    "spark = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data\n",
    "data_file = './data/bank-additional/bank-additional-full.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option(\"delimiter\", \";\").csv(data_file,header = True, inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41188"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('age', 'int'),\n",
       " ('job', 'string'),\n",
       " ('marital', 'string'),\n",
       " ('education', 'string'),\n",
       " ('default', 'string'),\n",
       " ('housing', 'string'),\n",
       " ('loan', 'string'),\n",
       " ('contact', 'string'),\n",
       " ('month', 'string'),\n",
       " ('day_of_week', 'string'),\n",
       " ('duration', 'int'),\n",
       " ('campaign', 'int'),\n",
       " ('pdays', 'int'),\n",
       " ('previous', 'int'),\n",
       " ('poutcome', 'string'),\n",
       " ('emp.var.rate', 'double'),\n",
       " ('cons.price.idx', 'double'),\n",
       " ('cons.conf.idx', 'double'),\n",
       " ('euribor3m', 'double'),\n",
       " ('nr.employed', 'double'),\n",
       " ('y', 'string')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Data explorations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## rename the column names\n",
    "df = df.toDF(*(c.replace('.', '_') for c in df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>emp_var_rate</th>\n",
       "      <th>cons_price_idx</th>\n",
       "      <th>cons_conf_idx</th>\n",
       "      <th>euribor3m</th>\n",
       "      <th>nr_employed</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56</td>\n",
       "      <td>housemaid</td>\n",
       "      <td>married</td>\n",
       "      <td>basic.4y</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>261</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>57</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>149</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>226</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>admin.</td>\n",
       "      <td>married</td>\n",
       "      <td>basic.6y</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>151</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>307</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age        job  marital    education  default housing loan    contact  \\\n",
       "0   56  housemaid  married     basic.4y       no      no   no  telephone   \n",
       "1   57   services  married  high.school  unknown      no   no  telephone   \n",
       "2   37   services  married  high.school       no     yes   no  telephone   \n",
       "3   40     admin.  married     basic.6y       no      no   no  telephone   \n",
       "4   56   services  married  high.school       no      no  yes  telephone   \n",
       "\n",
       "  month day_of_week  duration  campaign  pdays  previous     poutcome  \\\n",
       "0   may         mon       261         1    999         0  nonexistent   \n",
       "1   may         mon       149         1    999         0  nonexistent   \n",
       "2   may         mon       226         1    999         0  nonexistent   \n",
       "3   may         mon       151         1    999         0  nonexistent   \n",
       "4   may         mon       307         1    999         0  nonexistent   \n",
       "\n",
       "   emp_var_rate  cons_price_idx  cons_conf_idx  euribor3m  nr_employed   y  \n",
       "0           1.1          93.994          -36.4      4.857       5191.0  no  \n",
       "1           1.1          93.994          -36.4      4.857       5191.0  no  \n",
       "2           1.1          93.994          -36.4      4.857       5191.0  no  \n",
       "3           1.1          93.994          -36.4      4.857       5191.0  no  \n",
       "4           1.1          93.994          -36.4      4.857       5191.0  no  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>emp_var_rate</th>\n",
       "      <th>cons_price_idx</th>\n",
       "      <th>cons_conf_idx</th>\n",
       "      <th>euribor3m</th>\n",
       "      <th>nr_employed</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count</td>\n",
       "      <td>41188</td>\n",
       "      <td>41188</td>\n",
       "      <td>41188</td>\n",
       "      <td>41188</td>\n",
       "      <td>41188</td>\n",
       "      <td>41188</td>\n",
       "      <td>41188</td>\n",
       "      <td>41188</td>\n",
       "      <td>41188</td>\n",
       "      <td>41188</td>\n",
       "      <td>41188</td>\n",
       "      <td>41188</td>\n",
       "      <td>41188</td>\n",
       "      <td>41188</td>\n",
       "      <td>41188</td>\n",
       "      <td>41188</td>\n",
       "      <td>41188</td>\n",
       "      <td>41188</td>\n",
       "      <td>41188</td>\n",
       "      <td>41188</td>\n",
       "      <td>41188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean</td>\n",
       "      <td>40.02406040594348</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>258.2850101971448</td>\n",
       "      <td>2.567592502670681</td>\n",
       "      <td>962.4754540157328</td>\n",
       "      <td>0.17296299893172767</td>\n",
       "      <td>None</td>\n",
       "      <td>0.08188550063178392</td>\n",
       "      <td>93.57566436828918</td>\n",
       "      <td>-40.50260027191787</td>\n",
       "      <td>3.6212908128585366</td>\n",
       "      <td>5167.035910944004</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stddev</td>\n",
       "      <td>10.421249980934057</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>259.27924883646455</td>\n",
       "      <td>2.770013542902331</td>\n",
       "      <td>186.9109073447414</td>\n",
       "      <td>0.49490107983928927</td>\n",
       "      <td>None</td>\n",
       "      <td>1.57095974051703</td>\n",
       "      <td>0.5788400489541355</td>\n",
       "      <td>4.628197856174595</td>\n",
       "      <td>1.7344474048512557</td>\n",
       "      <td>72.25152766825924</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>min</td>\n",
       "      <td>17</td>\n",
       "      <td>admin.</td>\n",
       "      <td>divorced</td>\n",
       "      <td>basic.4y</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>apr</td>\n",
       "      <td>fri</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>failure</td>\n",
       "      <td>-3.4</td>\n",
       "      <td>92.201</td>\n",
       "      <td>-50.8</td>\n",
       "      <td>0.634</td>\n",
       "      <td>4963.6</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>max</td>\n",
       "      <td>98</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>telephone</td>\n",
       "      <td>sep</td>\n",
       "      <td>wed</td>\n",
       "      <td>4918</td>\n",
       "      <td>56</td>\n",
       "      <td>999</td>\n",
       "      <td>7</td>\n",
       "      <td>success</td>\n",
       "      <td>1.4</td>\n",
       "      <td>94.767</td>\n",
       "      <td>-26.9</td>\n",
       "      <td>5.045</td>\n",
       "      <td>5228.1</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  summary                 age      job   marital education default housing  \\\n",
       "0   count               41188    41188     41188     41188   41188   41188   \n",
       "1    mean   40.02406040594348     None      None      None    None    None   \n",
       "2  stddev  10.421249980934057     None      None      None    None    None   \n",
       "3     min                  17   admin.  divorced  basic.4y      no      no   \n",
       "4     max                  98  unknown   unknown   unknown     yes     yes   \n",
       "\n",
       "    loan    contact  month day_of_week            duration           campaign  \\\n",
       "0  41188      41188  41188       41188               41188              41188   \n",
       "1   None       None   None        None   258.2850101971448  2.567592502670681   \n",
       "2   None       None   None        None  259.27924883646455  2.770013542902331   \n",
       "3     no   cellular    apr         fri                   0                  1   \n",
       "4    yes  telephone    sep         wed                4918                 56   \n",
       "\n",
       "               pdays             previous poutcome         emp_var_rate  \\\n",
       "0              41188                41188    41188                41188   \n",
       "1  962.4754540157328  0.17296299893172767     None  0.08188550063178392   \n",
       "2  186.9109073447414  0.49490107983928927     None     1.57095974051703   \n",
       "3                  0                    0  failure                 -3.4   \n",
       "4                999                    7  success                  1.4   \n",
       "\n",
       "       cons_price_idx       cons_conf_idx           euribor3m  \\\n",
       "0               41188               41188               41188   \n",
       "1   93.57566436828918  -40.50260027191787  3.6212908128585366   \n",
       "2  0.5788400489541355   4.628197856174595  1.7344474048512557   \n",
       "3              92.201               -50.8               0.634   \n",
       "4              94.767               -26.9               5.045   \n",
       "\n",
       "         nr_employed      y  \n",
       "0              41188  41188  \n",
       "1  5167.035910944004   None  \n",
       "2  72.25152766825924   None  \n",
       "3             4963.6     no  \n",
       "4             5228.1    yes  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>no</td>\n",
       "      <td>36548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>yes</td>\n",
       "      <td>4640</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     y  count\n",
       "0   no  36548\n",
       "1  yes   4640"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupBy('y').count().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+---------+-------+-------+----+-------+-----+-----------+--------+--------+-----+--------+--------+------------+--------------+-------------+---------+-----------+---+\n",
      "|age|job|marital|education|default|housing|loan|contact|month|day_of_week|duration|campaign|pdays|previous|poutcome|emp_var_rate|cons_price_idx|cons_conf_idx|euribor3m|nr_employed|  y|\n",
      "+---+---+-------+---------+-------+-------+----+-------+-----+-----------+--------+--------+-----+--------+--------+------------+--------------+-------------+---------+-----------+---+\n",
      "|  0|  0|      0|        0|      0|      0|   0|      0|    0|          0|       0|       0|    0|       0|       0|           0|             0|            0|        0|          0|  0|\n",
      "+---+---+-------+---------+-------+-------+----+-------+-----+-----------+--------+--------+-----+--------+--------+------------+--------------+-------------+---------+-----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### check if missing values in the columns\n",
    "from pyspark.sql.functions import col,sum\n",
    "df.select(*(sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## select data type = string\n",
    "columnList = [item[0] for item in df.dtypes if item[1].startswith('string')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['job',\n",
       " 'marital',\n",
       " 'education',\n",
       " 'default',\n",
       " 'housing',\n",
       " 'loan',\n",
       " 'contact',\n",
       " 'month',\n",
       " 'day_of_week',\n",
       " 'poutcome',\n",
       " 'y']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columnList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### check each category's number of groups\n",
    "final_sum = []\n",
    "for col in columnList:\n",
    "    \n",
    "    #col = columnList[0]\n",
    "    summ = pd.DataFrame(df.select(col).groupby(col).count().orderBy(F.desc_nulls_first(\"count\")).collect(),columns=[\"value\",\"count\"]) \n",
    "    final_sum.append(summ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---------+-------+-------+----+-------+-----+-----------+--------+---+\n",
      "|job|marital|education|default|housing|loan|contact|month|day_of_week|poutcome|  y|\n",
      "+---+-------+---------+-------+-------+----+-------+-----+-----------+--------+---+\n",
      "| 12|      4|        8|      3|      3|   3|      2|   10|          5|       3|  2|\n",
      "+---+-------+---------+-------+-------+----+-------+-----+-----------+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, countDistinct\n",
    "\n",
    "df.agg(*(countDistinct(col(c)).alias(c) for c in columnList)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Train and test data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "fractions = df.select(\"y\").distinct().withColumn(\"fraction\", lit(0.8)).rdd.collectAsMap()\n",
    "df_train = df.sampleBy('y',fractions,seed=17)\n",
    "df_test = df.subtract(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age',\n",
       " 'job',\n",
       " 'marital',\n",
       " 'education',\n",
       " 'default',\n",
       " 'housing',\n",
       " 'loan',\n",
       " 'contact',\n",
       " 'month',\n",
       " 'day_of_week',\n",
       " 'duration',\n",
       " 'campaign',\n",
       " 'pdays',\n",
       " 'previous',\n",
       " 'poutcome',\n",
       " 'emp_var_rate',\n",
       " 'cons_price_idx',\n",
       " 'cons_conf_idx',\n",
       " 'euribor3m',\n",
       " 'nr_employed',\n",
       " 'y']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding and assembling\n",
    "encoding_var = [i[0] for i in df.dtypes if (i[1]=='string') & (i[0]!='y')]\n",
    "num_var = [i[0] for i in df.dtypes if ((i[1]=='int') | (i[1]=='double')) & (i[0]!='y')]\n",
    "\n",
    "'''from string to interger'''\n",
    "string_indexes = [StringIndexer(inputCol = c, outputCol = 'IDX_' + c, handleInvalid = 'keep') for c in encoding_var]\n",
    "'''from interger to binary vectors'''\n",
    "onehot_indexes = [OneHotEncoderEstimator(inputCols = ['IDX_' + c], outputCols = ['OHE_' + c]) for c in encoding_var]\n",
    "label_indexes = StringIndexer(inputCol = 'y', outputCol = 'label', handleInvalid = 'keep')\n",
    "\n",
    "\n",
    "## The input for the model should be binary vectors\n",
    "assembler = VectorAssembler(inputCols = num_var + ['OHE_' + c for c in encoding_var], outputCol = \"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Feature importances\n",
    "\n",
    "To avoid the overfitting issue, the feature selection was conducted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FeatureImportanceSelector import ExtractFeatureImp, FeatureImpSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "fi_pipe = Pipeline(stages = string_indexes + onehot_indexes + [assembler, label_indexes, rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## model fit\n",
    "mod = fi_pipe.fit(df_train)\n",
    "pred_train = mod.transform(df_train) ##get the predicted train values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>name</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>duration</td>\n",
       "      <td>0.218672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>nr_employed</td>\n",
       "      <td>0.170947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>pdays</td>\n",
       "      <td>0.162137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>62</td>\n",
       "      <td>OHE_poutcome_success</td>\n",
       "      <td>0.135583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>euribor3m</td>\n",
       "      <td>0.103517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>cons_conf_idx</td>\n",
       "      <td>0.048520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>cons_price_idx</td>\n",
       "      <td>0.031215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>60</td>\n",
       "      <td>OHE_poutcome_nonexistent</td>\n",
       "      <td>0.020316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>51</td>\n",
       "      <td>OHE_month_oct</td>\n",
       "      <td>0.015218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>52</td>\n",
       "      <td>OHE_month_sep</td>\n",
       "      <td>0.009130</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    idx                      name     score\n",
       "1     1                  duration  0.218672\n",
       "9     9               nr_employed  0.170947\n",
       "3     3                     pdays  0.162137\n",
       "62   62      OHE_poutcome_success  0.135583\n",
       "8     8                 euribor3m  0.103517\n",
       "7     7             cons_conf_idx  0.048520\n",
       "6     6            cons_price_idx  0.031215\n",
       "60   60  OHE_poutcome_nonexistent  0.020316\n",
       "51   51             OHE_month_oct  0.015218\n",
       "52   52             OHE_month_sep  0.009130"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ExtractFeatureImp(mod.stages[-1].featureImportances, pred_train, \"features\").head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Ensemble models\n",
    "\n",
    "### 4.1 Used all input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier, GBTClassifier,LogisticRegression, NaiveBayes\n",
    "\n",
    "rf = RandomForestClassifier(numTrees=20)\n",
    "xgb = GBTClassifier(maxIter= 10)\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.2)\n",
    "#nb = NaiveBayes(smoothing= 0.5, modelType=\"multinomial\")\n",
    "\n",
    "methods = {\"random forest\": rf,\n",
    "           \"logistic regression\": lr,\n",
    "          \"boosting tree\": xgb ##this needs to be different from others\n",
    "          #\"naive bayes\": nb\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9173651541898475\n",
      "0.8935541330413904\n",
      "0.9463653404925537\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "fitted_models ={}\n",
    "\n",
    "for method_name, method in methods.items():\n",
    "    \n",
    "    method.setPredictionCol(\"prediction_\" + method_name)\n",
    "    if method_name != \"boosting tree\":\n",
    "        method.setProbabilityCol(\"probability_\" + method_name)\n",
    "        method.setRawPredictionCol(\"raw_prediction_\" + method_name)\n",
    "        sel_col = \"probability_\" + method_name\n",
    "    else:\n",
    "        sel_col = \"probability\"\n",
    "    \n",
    "\n",
    "    pipe = Pipeline(stages = string_indexes + onehot_indexes + [assembler, label_indexes, method])\n",
    "    # need to keep fitted model somewhere\n",
    "    fitted_models[method_name] = pipe.fit(df_train)\n",
    "    df_test = fitted_models[method_name].transform(df_test)\n",
    "    \n",
    "    filter_col1 = [col for col in df_test.columns if col.startswith('IDX')]\n",
    "    filter_col2 = [col for col in df_test.columns if col.startswith('OHE')]\n",
    "    drop_cols = filter_col1 + filter_col2 + ['features','label']\n",
    "    \n",
    "    evaluator= BinaryClassificationEvaluator(rawPredictionCol=sel_col, metricName= \"areaUnderROC\")\n",
    "    print(evaluator.evaluate(df_test))\n",
    "    if method_name != list(methods.keys())[len(methods)-1]: ##if it is the last layer, we will not drop columns\n",
    "        df_test = df_test.drop(*drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>emp_var_rate</th>\n",
       "      <th>cons_price_idx</th>\n",
       "      <th>cons_conf_idx</th>\n",
       "      <th>euribor3m</th>\n",
       "      <th>nr_employed</th>\n",
       "      <th>y</th>\n",
       "      <th>raw_prediction_random forest</th>\n",
       "      <th>probability_random forest</th>\n",
       "      <th>prediction_random forest</th>\n",
       "      <th>raw_prediction_logistic regression</th>\n",
       "      <th>probability_logistic regression</th>\n",
       "      <th>prediction_logistic regression</th>\n",
       "      <th>IDX_job</th>\n",
       "      <th>IDX_marital</th>\n",
       "      <th>IDX_education</th>\n",
       "      <th>IDX_default</th>\n",
       "      <th>IDX_housing</th>\n",
       "      <th>IDX_loan</th>\n",
       "      <th>IDX_contact</th>\n",
       "      <th>IDX_month</th>\n",
       "      <th>IDX_day_of_week</th>\n",
       "      <th>IDX_poutcome</th>\n",
       "      <th>OHE_job</th>\n",
       "      <th>OHE_marital</th>\n",
       "      <th>OHE_education</th>\n",
       "      <th>OHE_default</th>\n",
       "      <th>OHE_housing</th>\n",
       "      <th>OHE_loan</th>\n",
       "      <th>OHE_contact</th>\n",
       "      <th>OHE_month</th>\n",
       "      <th>OHE_day_of_week</th>\n",
       "      <th>OHE_poutcome</th>\n",
       "      <th>features</th>\n",
       "      <th>label</th>\n",
       "      <th>rawPrediction</th>\n",
       "      <th>probability</th>\n",
       "      <th>prediction_boosting tree</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>retired</td>\n",
       "      <td>divorced</td>\n",
       "      <td>high.school</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>thu</td>\n",
       "      <td>223</td>\n",
       "      <td>2</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.855</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "      <td>[18.469669090507765, 1.5303309094922366, 0.0]</td>\n",
       "      <td>[0.9234834545253883, 0.07651654547461183, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[2.412882704278391, 0.33597119273934295, -4.69...</td>\n",
       "      <td>[0.8879918088105491, 0.11128016241138071, 0.00...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 1.0, 0.0)</td>\n",
       "      <td>(0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)</td>\n",
       "      <td>(1.0, 0.0, 0.0)</td>\n",
       "      <td>(1.0, 0.0, 0.0)</td>\n",
       "      <td>(1.0, 0.0, 0.0)</td>\n",
       "      <td>(0.0, 1.0)</td>\n",
       "      <td>(1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(1.0, 0.0, 0.0, 0.0, 0.0)</td>\n",
       "      <td>(1.0, 0.0, 0.0)</td>\n",
       "      <td>(60.0, 223.0, 2.0, 999.0, 0.0, 1.1, 93.994, -3...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[1.3070615799780285, -1.3070615799780285]</td>\n",
       "      <td>[0.9317650097070745, 0.06823499029292546]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26</td>\n",
       "      <td>student</td>\n",
       "      <td>single</td>\n",
       "      <td>university.degree</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>wed</td>\n",
       "      <td>217</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.859</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "      <td>[18.459292407360678, 1.5407075926393248, 0.0]</td>\n",
       "      <td>[0.9229646203680337, 0.07703537963196623, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[2.4128949176825527, 0.3359552684012981, -4.69...</td>\n",
       "      <td>[0.8879945178373172, 0.1112773707595281, 0.000...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 1.0, 0.0, 0.0)</td>\n",
       "      <td>(1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)</td>\n",
       "      <td>(1.0, 0.0, 0.0)</td>\n",
       "      <td>(1.0, 0.0, 0.0)</td>\n",
       "      <td>(1.0, 0.0, 0.0)</td>\n",
       "      <td>(0.0, 1.0)</td>\n",
       "      <td>(1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 1.0, 0.0, 0.0)</td>\n",
       "      <td>(1.0, 0.0, 0.0)</td>\n",
       "      <td>(26.0, 217.0, 1.0, 999.0, 0.0, 1.1, 93.994, -3...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[1.3119994885913469, -1.3119994885913469]</td>\n",
       "      <td>[0.9323902330034555, 0.06760976699654453]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>unknown</td>\n",
       "      <td>married</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>thu</td>\n",
       "      <td>270</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.860</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "      <td>[19.1173812019021, 0.8826187980979053, 0.0]</td>\n",
       "      <td>[0.9558690600951048, 0.04413093990489526, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[2.412796884645821, 0.3360900132153952, -4.693...</td>\n",
       "      <td>[0.8879714787209175, 0.11130038890590825, 0.00...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(1.0, 0.0, 0.0, 0.0)</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0)</td>\n",
       "      <td>(0.0, 1.0, 0.0)</td>\n",
       "      <td>(1.0, 0.0, 0.0)</td>\n",
       "      <td>(1.0, 0.0, 0.0)</td>\n",
       "      <td>(0.0, 1.0)</td>\n",
       "      <td>(1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(1.0, 0.0, 0.0, 0.0, 0.0)</td>\n",
       "      <td>(1.0, 0.0, 0.0)</td>\n",
       "      <td>(38.0, 270.0, 1.0, 999.0, 0.0, 1.1, 93.994, -3...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[1.3070615799780285, -1.3070615799780285]</td>\n",
       "      <td>[0.9317650097070745, 0.06823499029292546]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age      job   marital          education  default housing loan    contact  \\\n",
       "0   60  retired  divorced        high.school       no     yes   no  telephone   \n",
       "1   26  student    single  university.degree       no     yes   no  telephone   \n",
       "2   38  unknown   married            unknown  unknown     yes   no  telephone   \n",
       "\n",
       "  month day_of_week  duration  campaign  pdays  previous     poutcome  \\\n",
       "0   may         thu       223         2    999         0  nonexistent   \n",
       "1   may         wed       217         1    999         0  nonexistent   \n",
       "2   may         thu       270         1    999         0  nonexistent   \n",
       "\n",
       "   emp_var_rate  cons_price_idx  cons_conf_idx  euribor3m  nr_employed   y  \\\n",
       "0           1.1          93.994          -36.4      4.855       5191.0  no   \n",
       "1           1.1          93.994          -36.4      4.859       5191.0  no   \n",
       "2           1.1          93.994          -36.4      4.860       5191.0  no   \n",
       "\n",
       "                    raw_prediction_random forest  \\\n",
       "0  [18.469669090507765, 1.5303309094922366, 0.0]   \n",
       "1  [18.459292407360678, 1.5407075926393248, 0.0]   \n",
       "2    [19.1173812019021, 0.8826187980979053, 0.0]   \n",
       "\n",
       "                        probability_random forest  prediction_random forest  \\\n",
       "0  [0.9234834545253883, 0.07651654547461183, 0.0]                       0.0   \n",
       "1  [0.9229646203680337, 0.07703537963196623, 0.0]                       0.0   \n",
       "2  [0.9558690600951048, 0.04413093990489526, 0.0]                       0.0   \n",
       "\n",
       "                  raw_prediction_logistic regression  \\\n",
       "0  [2.412882704278391, 0.33597119273934295, -4.69...   \n",
       "1  [2.4128949176825527, 0.3359552684012981, -4.69...   \n",
       "2  [2.412796884645821, 0.3360900132153952, -4.693...   \n",
       "\n",
       "                     probability_logistic regression  \\\n",
       "0  [0.8879918088105491, 0.11128016241138071, 0.00...   \n",
       "1  [0.8879945178373172, 0.1112773707595281, 0.000...   \n",
       "2  [0.8879714787209175, 0.11130038890590825, 0.00...   \n",
       "\n",
       "   prediction_logistic regression  IDX_job  IDX_marital  IDX_education  \\\n",
       "0                             0.0      5.0          2.0            1.0   \n",
       "1                             0.0     10.0          1.0            0.0   \n",
       "2                             0.0     11.0          0.0            6.0   \n",
       "\n",
       "   IDX_default  IDX_housing  IDX_loan  IDX_contact  IDX_month  \\\n",
       "0          0.0          0.0       0.0          1.0        0.0   \n",
       "1          0.0          0.0       0.0          1.0        0.0   \n",
       "2          1.0          0.0       0.0          1.0        0.0   \n",
       "\n",
       "   IDX_day_of_week  IDX_poutcome  \\\n",
       "0              0.0           0.0   \n",
       "1              2.0           0.0   \n",
       "2              0.0           0.0   \n",
       "\n",
       "                                             OHE_job           OHE_marital  \\\n",
       "0  (0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...  (0.0, 0.0, 1.0, 0.0)   \n",
       "1  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  (0.0, 1.0, 0.0, 0.0)   \n",
       "2  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  (1.0, 0.0, 0.0, 0.0)   \n",
       "\n",
       "                              OHE_education      OHE_default      OHE_housing  \\\n",
       "0  (0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)  (1.0, 0.0, 0.0)  (1.0, 0.0, 0.0)   \n",
       "1  (1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)  (1.0, 0.0, 0.0)  (1.0, 0.0, 0.0)   \n",
       "2  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0)  (0.0, 1.0, 0.0)  (1.0, 0.0, 0.0)   \n",
       "\n",
       "          OHE_loan OHE_contact  \\\n",
       "0  (1.0, 0.0, 0.0)  (0.0, 1.0)   \n",
       "1  (1.0, 0.0, 0.0)  (0.0, 1.0)   \n",
       "2  (1.0, 0.0, 0.0)  (0.0, 1.0)   \n",
       "\n",
       "                                           OHE_month  \\\n",
       "0  (1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1  (1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2  (1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "             OHE_day_of_week     OHE_poutcome  \\\n",
       "0  (1.0, 0.0, 0.0, 0.0, 0.0)  (1.0, 0.0, 0.0)   \n",
       "1  (0.0, 0.0, 1.0, 0.0, 0.0)  (1.0, 0.0, 0.0)   \n",
       "2  (1.0, 0.0, 0.0, 0.0, 0.0)  (1.0, 0.0, 0.0)   \n",
       "\n",
       "                                            features  label  \\\n",
       "0  (60.0, 223.0, 2.0, 999.0, 0.0, 1.1, 93.994, -3...    0.0   \n",
       "1  (26.0, 217.0, 1.0, 999.0, 0.0, 1.1, 93.994, -3...    0.0   \n",
       "2  (38.0, 270.0, 1.0, 999.0, 0.0, 1.1, 93.994, -3...    0.0   \n",
       "\n",
       "                               rawPrediction  \\\n",
       "0  [1.3070615799780285, -1.3070615799780285]   \n",
       "1  [1.3119994885913469, -1.3119994885913469]   \n",
       "2  [1.3070615799780285, -1.3070615799780285]   \n",
       "\n",
       "                                 probability  prediction_boosting tree  \n",
       "0  [0.9317650097070745, 0.06823499029292546]                       0.0  \n",
       "1  [0.9323902330034555, 0.06760976699654453]                       0.0  \n",
       "2  [0.9317650097070745, 0.06823499029292546]                       0.0  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.limit(3).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second layer prediction\n",
    "\n",
    "To bulid a model on these probability columns and see how well it can predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_vars = [var for var in df_test.columns if var.startswith(\"probability\")]\n",
    "vs_second_layers = VectorAssembler(inputCols= prediction_vars, outputCol= \"second_layer_input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_layer = RandomForestClassifier(featuresCol= \"second_layer_input\", labelCol= \"label\",  probabilityCol = \"second_layer_output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier_b1589c9e4ad6"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To aviod existig column problems\n",
    "method_name = 'RF2'\n",
    "\n",
    "second_layer.setPredictionCol(\"prediction_\" + method_name)\n",
    "second_layer.setProbabilityCol(\"probability_\" + method_name)\n",
    "second_layer.setRawPredictionCol(\"raw_prediction_\" + method_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe1 = Pipeline(stages = [vs_second_layers, second_layer])\n",
    "\n",
    "model_second_layer = pipe1.fit(df_test)\n",
    "df_test1 =model_second_layer.transform(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator1= BinaryClassificationEvaluator(rawPredictionCol=\"probability_\" + method_name, metricName= \"areaUnderROC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9455766541155921\n"
     ]
    }
   ],
   "source": [
    "print(evaluator1.evaluate(df_test1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion matrix (as our data in fact is imblance,overall accuray is not engogh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = df_test1.select('label').toPandas().apply(lambda x : x[0], 1).values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_test = df_test1.select('label').toPandas().apply(lambda x : x[0], 1).values.tolist()\n",
    "y_test_pred = df_test1.select('prediction_RF2').toPandas().apply(lambda x : x[0], 1).values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_test = confusion_matrix(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7110,  269],\n",
       "       [ 398,  520]], dtype=int64)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Used Feature importance + ensemble models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "fractions = df.select(\"y\").distinct().withColumn(\"fraction\", lit(0.8)).rdd.collectAsMap()\n",
    "df_train2 = df.sampleBy('y',fractions,seed=17)\n",
    "df_test2 = df.subtract(df_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier, GBTClassifier,LogisticRegression, NaiveBayes\n",
    "\n",
    "rf = RandomForestClassifier(numTrees=20,labelCol=\"label\", featuresCol=\"features\")\n",
    "\n",
    "rf2 = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features_subset\")\n",
    "xgb2 = GBTClassifier(maxIter= 10,labelCol=\"label\", featuresCol=\"features_subset\")\n",
    "lr2 = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.2,labelCol=\"label\", featuresCol=\"features_subset\")\n",
    "#nb = NaiveBayes(smoothing= 0.5, modelType=\"multinomial\")\n",
    "\n",
    "methods = {\"random forest\": rf2,\n",
    "           \"logistic regression\": lr2,\n",
    "          \"boosting tree\": xgb2 ##this needs to be different from others\n",
    "          #\"naive bayes\": nb\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "##feature selection, which include rf. thus, no need to put it in the pipeline\n",
    "fis = FeatureImpSelector(estimator = rf, selectorType = \"numTopFeatures\",\n",
    "                         numTopFeatures = 7, outputCol = \"features_subset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9283402584204542\n",
      "0.9205904644310935\n",
      "0.9451353292819138\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "fitted_models2 ={}\n",
    "\n",
    "for method_name, method in methods.items():\n",
    "    \n",
    "    method.setPredictionCol(\"prediction_\" + method_name)\n",
    "    if method_name != \"boosting tree\":\n",
    "        method.setProbabilityCol(\"probability_\" + method_name)\n",
    "        method.setRawPredictionCol(\"raw_prediction_\" + method_name)\n",
    "        sel_col = \"probability_\" + method_name\n",
    "    else:\n",
    "        sel_col = \"probability\"\n",
    "    \n",
    "\n",
    "    pipe2 = Pipeline(stages = string_indexes + onehot_indexes + [assembler, label_indexes,fis, method])\n",
    "    # need to keep fitted model somewhere\n",
    "    fitted_models2[method_name] = pipe2.fit(df_train2)\n",
    "    df_test2 = fitted_models2[method_name].transform(df_test2)\n",
    "    \n",
    "    filter_col1 = [col for col in df_test1.columns if col.startswith('IDX')]\n",
    "    filter_col2 = [col for col in df_test1.columns if col.startswith('OHE')]\n",
    "    drop_cols = filter_col1 + filter_col2 + ['features','features_subset','label']\n",
    "    \n",
    "    evaluator= BinaryClassificationEvaluator(rawPredictionCol=sel_col, metricName= \"areaUnderROC\")\n",
    "    print(evaluator.evaluate(df_test2))\n",
    "    if method_name != list(methods.keys())[len(methods)-1]: ##if it is the last layer, we will not drop columns\n",
    "        df_test2 = df_test2.drop(*drop_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second layer prediction\n",
    "\n",
    "To bulid a model on these probability columns and see how well it can predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_vars2 = [var for var in df_test2.columns if var.startswith(\"probability\")]\n",
    "vs_second_layers2 = VectorAssembler(inputCols= prediction_vars2, outputCol= \"second_layer_input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_layer2 = RandomForestClassifier(featuresCol= \"second_layer_input\", labelCol= \"label\",  probabilityCol = \"second_layer_output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier_42e25de3b51f"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To aviod existig column problems\n",
    "method_name = 'RF22'\n",
    "\n",
    "second_layer2.setPredictionCol(\"prediction_\" + method_name)\n",
    "second_layer2.setProbabilityCol(\"probability_\" + method_name)\n",
    "second_layer2.setRawPredictionCol(\"raw_prediction_\" + method_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe2 = Pipeline(stages = [vs_second_layers2, second_layer2])\n",
    "\n",
    "model_second_layer2 = pipe2.fit(df_test2)\n",
    "df_test22 =model_second_layer2.transform(df_test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator2 = BinaryClassificationEvaluator(rawPredictionCol=\"probability_\" + method_name, metricName= \"areaUnderROC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9454093950299399\n"
     ]
    }
   ],
   "source": [
    "print(evaluator2.evaluate(df_test22))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test2 = df_test22.select('label').toPandas().apply(lambda x : x[0], 1).values.tolist()\n",
    "y_test_pred2 = df_test22.select('prediction_RF22').toPandas().apply(lambda x : x[0], 1).values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7162,  217],\n",
       "       [ 420,  498]], dtype=int64)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test2, y_test_pred2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems adding the feature selection did not improve the model performance in this case. Of course, there are other parameters such as the number of paramters from the feature selection step. Another factor which significantly affect the model performance will be the imblance data. The minorty group didn't captur well with the current methods.\n",
    "\n",
    "The ensemble model can make our results become more stable as three methods both give us very similar performance, but it did not improve the final classification results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
